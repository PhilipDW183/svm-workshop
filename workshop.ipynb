{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS07 Data Science with Python - Support Vector Machine\n",
    "In this workshop we continue using the Sci-Kit Learn library. This time we look into the support vector machine.\n",
    "### Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data \n",
    "The two code cells below generate a data frame from the \"RedBlue.csv\" file and plot the data. Run both these code cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a data frame from the \"RedBlue.csv\" file\n",
    "df = pd.read_csv(\"RedBlue.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits rows based on the colour in the \"Colour\" column\n",
    "red = df.loc[df[\"Colour\"] == \"red\"]\n",
    "blue =  df.loc[df[\"Colour\"] == \"blue\"]\n",
    "\n",
    "# Plots the red data and the blue data\n",
    "plt.plot(red[\"x\"],red[\"y\"],\"r+\")\n",
    "plt.plot(blue[\"x\"],blue[\"y\"],\"b+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "As you can see in the plot above we have data seperated into two sets. In this data set there is a red set and a blue set. More generally we could have other ways of distinguishing two groups ie the two groups could represent two different blood types for example or perhaps whether a particular project is successful or not. What if we had another point or set of points that were not yet assigned to red or blue? How would we determine which group they belong to? One could do this by eye of course but if you had to many points to classify this is not ideal. Do we have a better more quantitative way classifying these points? In this workshop we will explore one such approach to classifying these data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine (SVM)\n",
    "As an alternative to the K nearest neighbours algorithm we could use a support vector machine or SVM for short. Once again we seek to classify our data in some way. A SVM works by finding a boundary that seperates your two data sets. In the case above it finds the straight line that is as far as possible from both the blue and red data sets as it can be. When given an unseen point, the SVM checks to see which side of the line the data appears on. If the data is on the side featuring the blue data it will classify the point blue. If on the side with the red data the SVM assigns it to the red group. \n",
    "### Preprocessing \n",
    "The preprocessing stage here is identical to that used for the K nearest neighbour algorithm in workshop DS06."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables\n",
    "X = df.drop(\"Colour\", axis=1)\n",
    "\n",
    "# Dependent variable \n",
    "y = df[\"Colour\"]\n",
    "\n",
    "# Allocates 80% of the data for training and 20% for testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Scoring\n",
    "Once again, very similar to the K nearest neighbours algorithm. The only difference we've made is in how we've defined the c initially by choosing a different algorithm. Here we define it ````svm.SVC```` where the SVC stands for support vector classifier. The ````gamma='auto'```` argument specifies a coefficient for whats called a kernel. We will discuss kernals later in the workshop. Training for an SVM involves determining where the seperating boundary or line in this case between the two groups is. Making a prediction merely involves determining the position of the unseen data with respect to this boundary. Our score should once again be $1$ as our data groups are easily distinguishable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chooses the support vector machine algorithm for our classifier\n",
    "clf = svm.SVC(gamma='auto')\n",
    "\n",
    "\n",
    "# Training the classifier\n",
    "clf_trained = clf.fit(X_train,y_train)\n",
    "\n",
    "# Scoring the classifier\n",
    "clf_trained.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "Here we will pass a few unseen test points into our classifier to see what it predicts.\n",
    "#### [10,10]\n",
    "Quite clearly in the blue region so our model should return blue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an unclassified point \n",
    "test_point = np.array([[10,10]])\n",
    "\n",
    "# Making a prediction\n",
    "clf_trained.predict(test_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [5,5]\n",
    "Quite clearly in the red region so our model should return red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an unclassified point \n",
    "test_point = np.array([[5,5]])\n",
    "\n",
    "# Making a prediction\n",
    "clf_trained.predict(test_point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More than 2 Classes?\n",
    "In the data set above we were only considered two classes of data. What would happen if we had to classify between three groups of data or more? Below we consider a set of data with a third group. We apply the support vector machine in exactly the same way, however this time we will consider an additional argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates data frame from csv\n",
    "df = pd.read_csv(\"RedBlueGreen.csv\")\n",
    "\n",
    "# Splitting the colours\n",
    "red = df.loc[df[\"Colours\"] == \"red\"]\n",
    "blue = df.loc[df[\"Colours\"] == \"blue\"]\n",
    "green = df.loc[df[\"Colours\"] == \"green\"]\n",
    "\n",
    "# Plotting\n",
    "plt.plot(green[\"x\"],green[\"y\"],\"g+\")\n",
    "plt.plot(blue[\"x\"],blue[\"y\"],\"b+\")\n",
    "plt.plot(red[\"x\"],red[\"y\"],\"r+\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this data is in three groups we cannot simply find one line that seperates the groups. In this case we will need to find multiple lines. There are two methods of doing so. The first is to consider one group at a time whilst treating the rest of the data as if it were one whole group. This is called one vs rest. We do this for each group so that we have a line seperating that group and the rest of the data. Alternatively, we can consider the boundary between each individual group. In this case we could consider red vs green then red vs blue and blue vs green finding a line seperating each case. This is called one vs one. We can specify which of these methods to choose when passing ````sv.SVC```` by using the ````decision_function_shape```` argument. Setting it to ````\"ovo\"```` selects the one vs one method. By default it is set to ````ovr```` which corresponds to one vs rest. Often it can be useful to test the outcomes of both these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables\n",
    "X = df.drop(\"Colours\", axis=1) \n",
    "\n",
    "# Dependent variable\n",
    "y = df[\"Colours\"]\n",
    "\n",
    "# Allocating training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "# Choosing support vector machine for classifier\n",
    "clf = svm.SVC(gamma=\"auto\", decision_function_shape = \"ovo\")\n",
    "\n",
    "# Training\n",
    "clf_trained = clf.fit(X_train,y_train)\n",
    "\n",
    "# Scoring\n",
    "clf_trained.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kernels\n",
    "The data sets we used in this workshop are very idealised examples. The data can easily be split into groups with a straight lines. But what about data posisitioned in a way that you would not be able to find a straight line between the groups. For instance, if I had one group in filling a circle and another group around that circle then no straight line would be able to seperate the data. In such cases we might be able to map our data to higher dimensions with what are called kernels. In these higher dimensions we might be able to find a straight surface that seperates the two groups. One can choose a kernal by passing the ````kernel=```` argument into ````svm.SVC````. The available options for this argument are ````'linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’```` with ````'rbf'```` being the default. Don't worry too much about how they work but be aware they exist and that changing the kernel may improve or reduce the accuracy of your model. The following link provides a visualisation as to how a kernel might find a straight boundary that can seperate two seemingly inseperable groups: https://www.youtube.com/watch?v=3liCbRZPrZA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = pd.read_excel(\"cycleaccidents.xlsx\")\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset above represents the safety level of pedal cycle accidents / traffic in 2018 by 3 different conditions. This is  the ````casualties/milkmtrafic2```` column. The ````Local authority```` column represents the London local authority where that safety lavel is recorded. The ````Light condition````column represents the light condition where that safety level is recorded. The ````Weather condition```` column represents the weather where that safety level was recorded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to predict the safety level, ````casualties/milkmtrafic````, using our 3 predictors, we couldn't as the 3 columns are categorical and not numerical. However,it is possible to use them if we use label encoding. All the categories will be assign a number. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by making duplicates of each predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label['Local authority1']=label['Local authority']\n",
    "label['Weather condition1']=label['Weather condition']\n",
    "label['Light condition1']=label['Light condition']\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create an array with the columns we want labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode=['Local authority','Light condition','Weather condition']\n",
    "encode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import Label Encoder and apply it to our 3 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "# Apply label encoder\n",
    "label_encoder = LabelEncoder()\n",
    "for col in set(encode):\n",
    "    label[col] = label_encoder.fit_transform(label[col])\n",
    "\n",
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we previously created duplicates, now you can see both the encoded and the original column. We can now use the encoded column to train a SVM to predict safety levels. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
